<p align="center">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/xuhuizhan5/xuhuizhan5/output/github-contribution-grid-snake-dark.svg" />
    <source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/xuhuizhan5/xuhuizhan5/output/github-contribution-grid-snake-light.svg" />
    <img alt="GitHub Contribution Snake" src="https://raw.githubusercontent.com/xuhuizhan5/xuhuizhan5/output/github-contribution-grid-snake.svg" />
  </picture>
</p>

<h1 align="center">Hi, I'm Daniel (Zhan) 🧪</h1>
<h3 align="center">Machine Learning Engineer · Software Development Engineer · Machine Learning Researcher</h3>

<p align="center">
  <a href="https://xuhuizhan5.github.io">🌐 Portfolio</a> •
  <a href="https://www.linkedin.com/in/danielzhandatascience/">🔗 LinkedIn</a>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Focus-Multi--Modality%20%7C%20Representation%20Learning-blueviolet?style=flat-square" />
  <img src="https://img.shields.io/badge/Core-Python%20%7C%20Go-green?style=flat-square" />
  <img src="https://img.shields.io/badge/Frameworks-PyTorch%20%7C%20JAX%20(on%20deck)-ff69b4?style=flat-square" />
  <img src="https://img.shields.io/badge/Infra-AWS%20%7C%20Kubernetes-orange?style=flat-square" />
</p>

---

```text
> initializing inverse-fusion
  loading vision encoder ... ok
  loading language encoder ... ok
  skipping alignment pretraining ... ✅
  constructing inverse projection graph ... done
  status: READY FOR REASONING
```

---

### 🚀 What I Do
I turn research prototypes into robust, scalable systems at the intersection of ML and engineering.

- Alignment-free multi-modal model design
- Preserving modality identity during fusion
- Efficient training and deployable inference pathways
- Clean abstractions bridging research and production

---

### 🔬 Spotlight: Inverse-LLaVA
A novel multi-modality framework that removes the alignment pre-training stage by inverting the common projection paradigm. Instead of forcing modalities into a homogenized latent early, each retains its native structure until a selective fusion core resolves shared semantics.

Core principles:
- Inverse projection
- Fidelity preservation over forced alignment
- Modular fusion kernels
---

### 🛠 Stack & Tools
- Languages: Python, Go
- ML Frameworks: PyTorch (primary), JAX (on deck)
- Infra / Ops: AWS, Kubernetes
- Patterns: Multi-modality, representation learning, efficiency

---

### 🧪 Current Focus
- Alternative fusion strategies with minimal pre-alignment
- Evaluating representation drift under partial supervision
- Lightweight reasoning layers atop preserved modality embeddings

---

### 💡 Philosophy
Bigger isn’t always smarter. Architectural restraint plus modality integrity can outperform brute-force pretraining when guided by principled interfaces.

---

### 🎨 Fun Corner
```text
Inverse Fusion Status: ACTIVE
Entropy Budget: within tolerance
Curiosity Engine: overheated (cooldown not initiated)
```

---

### 🤝 Let’s Connect
- Portfolio: [xuhuizhan5.github.io](https://xuhuizhan5.github.io)
- LinkedIn: [Daniel Zhan](https://www.linkedin.com/in/danielzhandatascience/)
