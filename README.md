<p align="center">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/xuhuizhan5/xuhuizhan5/output/github-contribution-grid-snake-dark.svg" />
    <source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/xuhuizhan5/xuhuizhan5/output/github-contribution-grid-snake-light.svg" />
    <img alt="GitHub Contribution Snake" src="https://raw.githubusercontent.com/xuhuizhan5/xuhuizhan5/output/github-contribution-grid-snake.svg" />
  </picture>
</p>

<h1 align="center">Hi, I'm Daniel (Zhan) ğŸ§ª</h1>
<h3 align="center">Machine Learning Engineer Â· Software Development Engineer Â· Machine Learning Researcher</h3>

<p align="center">
  <a href="https://xuhuizhan5.github.io">ğŸŒ Portfolio</a> â€¢
  <a href="https://www.linkedin.com/in/danielzhandatascience/">ğŸ”— LinkedIn</a>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Focus-Multi--Modality%20%7C%20Representation%20Learning-blueviolet?style=flat-square" />
  <img src="https://img.shields.io/badge/Core-Python%20%7C%20Go-green?style=flat-square" />
  <img src="https://img.shields.io/badge/Frameworks-PyTorch%20%7C%20JAX%20(on%20deck)-ff69b4?style=flat-square" />
  <img src="https://img.shields.io/badge/Infra-AWS%20%7C%20Kubernetes-orange?style=flat-square" />
</p>

---

```text
> initializing inverse-fusion
  loading vision encoder ... ok
  loading language encoder ... ok
  skipping alignment pretraining ... âœ…
  constructing inverse projection graph ... done
  status: READY FOR REASONING
```

---

### ğŸš€ What I Do
I turn research prototypes into robust, scalable systems at the intersection of ML and engineering.

- Alignment-free multi-modal model design
- Preserving modality identity during fusion
- Efficient training and deployable inference pathways
- Clean abstractions bridging research and production

---

### ğŸ”¬ Spotlight: Inverse-LLaVA
A novel multi-modality framework that removes the alignment pre-training stage by inverting the common projection paradigm. Instead of forcing modalities into a homogenized latent early, each retains its native structure until a selective fusion core resolves shared semantics.

Core principles:
- Inverse projection
- Fidelity preservation over forced alignment
- Modular fusion kernels
---

### ğŸ›  Stack & Tools
- Languages: Python, Go
- ML Frameworks: PyTorch (primary), JAX (on deck)
- Infra / Ops: AWS, Kubernetes
- Patterns: Multi-modality, representation learning, efficiency

---

### ğŸ§ª Current Focus
- Alternative fusion strategies with minimal pre-alignment
- Evaluating representation drift under partial supervision
- Lightweight reasoning layers atop preserved modality embeddings

---

### ğŸ’¡ Philosophy
Bigger isnâ€™t always smarter. Architectural restraint plus modality integrity can outperform brute-force pretraining when guided by principled interfaces.

---

### ğŸ¨ Fun Corner
```text
Inverse Fusion Status: ACTIVE
Entropy Budget: within tolerance
Curiosity Engine: overheated (cooldown not initiated)
```

---

### ğŸ¤ Letâ€™s Connect
- Portfolio: [xuhuizhan5.github.io](https://xuhuizhan5.github.io)
- LinkedIn: [Daniel Zhan](https://www.linkedin.com/in/danielzhandatascience/)
